# Llama-2_from_sratch
Full coding of LLaMA 2 from scratch, including Rotary Positional Embedding, RMS Normalization, Multi-Query Attention, KV Cache, Grouped Query Attention (GQA), the SwiGLU Activation function and more!
